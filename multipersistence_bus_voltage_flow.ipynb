{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Multipersistence Analysis on Bus Data\n",
        "\n",
        "This notebook implements multipersistence topology on bus data using:\n",
        "- **Node filtration**: Voltage (proxied by TotConnected_ActiveLoad)\n",
        "- **Edge features**: Branch flow (proxied by capacity)\n",
        "- **Classifier**: XGBoost\n",
        "- **Thresholds**: 10-20 thresholds based on quartiles\n",
        "\n",
        "## Overview\n",
        "Multipersistence extends traditional persistent homology to multiple parameters simultaneously, allowing us to capture topological features that depend on both node and edge attributes.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. Import Required Modules\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import networkx as nx\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Topological data analysis\n",
        "from gtda.homology import VietorisRipsPersistence\n",
        "from gtda.diagrams import BettiCurve\n",
        "import pyflagser\n",
        "\n",
        "# Machine learning\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
        "from sklearn.metrics import accuracy_score, balanced_accuracy_score, roc_auc_score, confusion_matrix\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Graph processing\n",
        "import xml.etree.ElementTree as ET\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"All modules imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Load and Parse Bus Data\n",
        "\n",
        "We'll parse the GML file to extract nodes and edges with their attributes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def parse_gml_file(filename):\n",
        "    \"\"\"\n",
        "    Parse GML file and extract graph structure with node and edge attributes.\n",
        "    \"\"\"\n",
        "    # Read the GML file\n",
        "    with open(filename, 'r') as f:\n",
        "        content = f.read()\n",
        "    \n",
        "    # Parse nodes\n",
        "    nodes = {}\n",
        "    edges = []\n",
        "    \n",
        "    lines = content.split('\\n')\n",
        "    i = 0\n",
        "    \n",
        "    # Extract nodes\n",
        "    while i < len(lines):\n",
        "        line = lines[i].strip()\n",
        "        if line == 'node [':\n",
        "            node_data = {}\n",
        "            i += 1\n",
        "            while i < len(lines) and lines[i].strip() != ']':\n",
        "                line = lines[i].strip()\n",
        "                if ' ' in line and not line.startswith('[') and not line.startswith(']'):\n",
        "                    key, value = line.split(' ', 1)\n",
        "                    # Try to convert to float if possible\n",
        "                    try:\n",
        "                        node_data[key] = float(value)\n",
        "                    except ValueError:\n",
        "                        node_data[key] = value.strip('\"')\n",
        "                i += 1\n",
        "            nodes[node_data['id']] = node_data\n",
        "        elif line == 'edge [':\n",
        "            edge_data = {}\n",
        "            i += 1\n",
        "            while i < len(lines) and lines[i].strip() != ']':\n",
        "                line = lines[i].strip()\n",
        "                if ' ' in line and not line.startswith('[') and not line.startswith(']'):\n",
        "                    key, value = line.split(' ', 1)\n",
        "                    # Try to convert to float if possible\n",
        "                    try:\n",
        "                        edge_data[key] = float(value)\n",
        "                    except ValueError:\n",
        "                        edge_data[key] = value.strip('\"')\n",
        "                i += 1\n",
        "            edges.append(edge_data)\n",
        "        i += 1\n",
        "    \n",
        "    return nodes, edges\n",
        "\n",
        "# Load bus data\n",
        "nodes, edges = parse_gml_file('bus37Ex.gml')\n",
        "\n",
        "print(f\"Loaded {len(nodes)} nodes and {len(edges)} edges\")\n",
        "print(\"\\nSample node attributes:\")\n",
        "for node_id, attrs in list(nodes.items())[:3]:\n",
        "    print(f\"Node {node_id}: {attrs}\")\n",
        "\n",
        "print(\"\\nSample edge attributes:\")\n",
        "for i, edge in enumerate(edges[:3]):\n",
        "    print(f\"Edge {i}: {edge}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. Construct NetworkX Graph with Attributes\n",
        "\n",
        "We'll create a NetworkX graph and assign node/edge attributes for multipersistence analysis.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def construct_graph_with_attributes(nodes, edges):\n",
        "    \"\"\"\n",
        "    Construct NetworkX graph with node and edge attributes.\n",
        "    \"\"\"\n",
        "    G = nx.Graph()\n",
        "    \n",
        "    # Add nodes with attributes\n",
        "    for node_id, attrs in nodes.items():\n",
        "        # Use TotConnected_ActiveLoad as proxy for voltage\n",
        "        voltage_proxy = attrs.get('TotConnected_ActiveLoad', 0.0)\n",
        "        G.add_node(node_id, voltage=voltage_proxy, **attrs)\n",
        "    \n",
        "    # Add edges with attributes\n",
        "    for edge in edges:\n",
        "        source = int(edge['source'])\n",
        "        target = int(edge['target'])\n",
        "        # Use 'cap' as proxy for branch flow\n",
        "        branch_flow_proxy = edge.get('cap', 0.0)\n",
        "        G.add_edge(source, target, branch_flow=branch_flow_proxy, **edge)\n",
        "    \n",
        "    return G\n",
        "\n",
        "# Construct the graph\n",
        "G = construct_graph_with_attributes(nodes, edges)\n",
        "\n",
        "print(f\"Graph constructed with {G.number_of_nodes()} nodes and {G.number_of_edges()} edges\")\n",
        "\n",
        "# Extract voltage and branch flow values for analysis\n",
        "voltage_values = [G.nodes[n]['voltage'] for n in G.nodes()]\n",
        "branch_flow_values = [G.edges[e]['branch_flow'] for e in G.edges()]\n",
        "\n",
        "print(f\"\\nVoltage (ActiveLoad) statistics:\")\n",
        "print(f\"  Min: {min(voltage_values):.2f}\")\n",
        "print(f\"  Max: {max(voltage_values):.2f}\")\n",
        "print(f\"  Mean: {np.mean(voltage_values):.2f}\")\n",
        "print(f\"  Std: {np.std(voltage_values):.2f}\")\n",
        "\n",
        "print(f\"\\nBranch Flow (Capacity) statistics:\")\n",
        "print(f\"  Min: {min(branch_flow_values):.2f}\")\n",
        "print(f\"  Max: {max(branch_flow_values):.2f}\")\n",
        "print(f\"  Mean: {np.mean(branch_flow_values):.2f}\")\n",
        "print(f\"  Std: {np.std(branch_flow_values):.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4. Generate Synthetic Labels for Classification\n",
        "\n",
        "Since the bus data doesn't have ground truth labels, we'll create synthetic labels based on network topology characteristics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_synthetic_labels(graph, n_samples=50):\n",
        "    \"\"\"\n",
        "    Generate synthetic labels based on network characteristics.\n",
        "    We'll create multiple 'scenarios' by perturbing the original graph.\n",
        "    \"\"\"\n",
        "    graphs = []\n",
        "    labels = []\n",
        "    \n",
        "    # Original graph as class 0\n",
        "    graphs.append(graph.copy())\n",
        "    labels.append(0)\n",
        "    \n",
        "    # Generate perturbed versions as class 1\n",
        "    np.random.seed(42)\n",
        "    for i in range(n_samples - 1):\n",
        "        G_perturbed = graph.copy()\n",
        "        \n",
        "        # Randomly perturb node voltages\n",
        "        for node in G_perturbed.nodes():\n",
        "            noise = np.random.normal(0, 0.1 * G_perturbed.nodes[node]['voltage'])\n",
        "            G_perturbed.nodes[node]['voltage'] = max(0, G_perturbed.nodes[node]['voltage'] + noise)\n",
        "        \n",
        "        # Randomly perturb edge flows\n",
        "        for edge in G_perturbed.edges():\n",
        "            noise = np.random.normal(0, 0.1 * G_perturbed.edges[edge]['branch_flow'])\n",
        "            G_perturbed.edges[edge]['branch_flow'] = max(0, G_perturbed.edges[edge]['branch_flow'] + noise)\n",
        "        \n",
        "        graphs.append(G_perturbed)\n",
        "        labels.append(1)\n",
        "    \n",
        "    return graphs, labels\n",
        "\n",
        "# Generate synthetic dataset\n",
        "graphs, labels = generate_synthetic_labels(G, n_samples=100)\n",
        "\n",
        "print(f\"Generated {len(graphs)} graphs with labels: {np.bincount(labels)}\")\n",
        "print(f\"Class distribution: {np.bincount(labels) / len(labels)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5. Multipersistence Pipeline Setup\n",
        "\n",
        "We'll implement multipersistence using voltage for node filtration and branch flow for edge features.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_thresholds(values, n_thresholds, use_quantiles=True):\n",
        "    \"\"\"\n",
        "    Build sorted unique threshold array for given values.\n",
        "    \"\"\"\n",
        "    vals = np.asarray(values)\n",
        "    if vals.size == 0:\n",
        "        return np.array([0.0])\n",
        "    if use_quantiles:\n",
        "        qs = np.linspace(0.0, 1.0, n_thresholds)\n",
        "        thr = np.quantile(vals, qs)\n",
        "    else:\n",
        "        thr = np.linspace(vals.min(), vals.max(), n_thresholds)\n",
        "    return np.unique(thr)\n",
        "\n",
        "def compute_betti_numbers_for_active(active_nodes, graph):\n",
        "    \"\"\"\n",
        "    Compute Betti numbers for active nodes using pyflagser.\n",
        "    \"\"\"\n",
        "    if len(active_nodes) == 0:\n",
        "        return (0, 0, 0)\n",
        "    \n",
        "    # Create subgraph with active nodes\n",
        "    subgraph = graph.subgraph(active_nodes)\n",
        "    \n",
        "    if subgraph.number_of_nodes() == 0:\n",
        "        return (0, 0, 0)\n",
        "    \n",
        "    # Convert to adjacency matrix\n",
        "    adj_matrix = nx.adjacency_matrix(subgraph).toarray().astype(np.float32)\n",
        "    \n",
        "    try:\n",
        "        # Compute persistence using pyflagser\n",
        "        result = pyflagser.flagser_unweighted(adj_matrix, directed=False)\n",
        "        \n",
        "        # Extract Betti numbers\n",
        "        betti_0 = result['betti'][0] if len(result['betti']) > 0 else 0\n",
        "        betti_1 = result['betti'][1] if len(result['betti']) > 1 else 0\n",
        "        betti_2 = result['betti'][2] if len(result['betti']) > 2 else 0\n",
        "        \n",
        "        return (betti_0, betti_1, betti_2)\n",
        "    except:\n",
        "        # Fallback: simple computation\n",
        "        n_components = nx.number_connected_components(subgraph)\n",
        "        n_edges = subgraph.number_of_edges()\n",
        "        n_nodes = subgraph.number_of_nodes()\n",
        "        \n",
        "        betti_0 = n_components\n",
        "        betti_1 = max(0, n_edges - n_nodes + n_components)\n",
        "        betti_2 = 0  # Simplified\n",
        "        \n",
        "        return (betti_0, betti_1, betti_2)\n",
        "\n",
        "def vectorize_betti_surfaces(betti0, betti1, betti2):\n",
        "    \"\"\"\n",
        "    Vectorize 2D Betti surfaces into 1D feature vector.\n",
        "    \"\"\"\n",
        "    f = []\n",
        "    for surf in (betti0, betti1, betti2):\n",
        "        flat = surf.flatten()\n",
        "        f.extend(flat.tolist())\n",
        "        # Add summary statistics\n",
        "        f.append(np.mean(flat))\n",
        "        f.append(np.max(flat))\n",
        "        f.append(np.sum(flat))\n",
        "        f.append(np.var(flat))\n",
        "    return np.array(f, dtype=float)\n",
        "\n",
        "print(\"Multipersistence pipeline functions defined successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6. Compute Multipersistence Features\n",
        "\n",
        "We'll compute multipersistence surfaces for each graph using voltage and branch flow thresholds.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "N_THRESH_VOLTAGE = 15  # Number of voltage thresholds\n",
        "N_THRESH_FLOW = 15     # Number of branch flow thresholds\n",
        "USE_QUANTILES = True   # Use quantile-based thresholds\n",
        "\n",
        "# Collect all voltage and flow values across all graphs\n",
        "all_voltage_vals = []\n",
        "all_flow_vals = []\n",
        "\n",
        "for graph in graphs:\n",
        "    all_voltage_vals.extend([graph.nodes[n]['voltage'] for n in graph.nodes()])\n",
        "    all_flow_vals.extend([graph.edges[e]['branch_flow'] for e in graph.edges()])\n",
        "\n",
        "# Build thresholds\n",
        "thr_voltage = build_thresholds(all_voltage_vals, N_THRESH_VOLTAGE, use_quantiles=USE_QUANTILES)\n",
        "thr_flow = build_thresholds(all_flow_vals, N_THRESH_FLOW, use_quantiles=USE_QUANTILES)\n",
        "\n",
        "print(f\"Voltage thresholds ({len(thr_voltage)}): {thr_voltage[:5]}...{thr_voltage[-3:]}\")\n",
        "print(f\"Flow thresholds ({len(thr_flow)}): {thr_flow[:5]}...{thr_flow[-3:]}\")\n",
        "\n",
        "# Compute multipersistence features\n",
        "features = []\n",
        "\n",
        "for graph in tqdm(graphs, desc=\"Computing multipersistence surfaces\"):\n",
        "    nV = len(thr_voltage)\n",
        "    nF = len(thr_flow)\n",
        "    \n",
        "    betti0_surf = np.zeros((nV, nF), dtype=int)\n",
        "    betti1_surf = np.zeros((nV, nF), dtype=int)\n",
        "    betti2_surf = np.zeros((nV, nF), dtype=int)\n",
        "    \n",
        "    # Prefetch node and edge attributes\n",
        "    node_voltage = {n: graph.nodes[n]['voltage'] for n in graph.nodes()}\n",
        "    edge_flow = {e: graph.edges[e]['branch_flow'] for e in graph.edges()}\n",
        "    \n",
        "    for iV, v_thresh in enumerate(thr_voltage):\n",
        "        for iF, f_thresh in enumerate(thr_flow):\n",
        "            # Sublevel-sublevel filtration: nodes with voltage <= v_thresh AND edges with flow <= f_thresh\n",
        "            active_nodes = []\n",
        "            for node in graph.nodes():\n",
        "                if node_voltage[node] <= v_thresh:\n",
        "                    # Check if all incident edges have flow <= f_thresh\n",
        "                    incident_edges = list(graph.edges(node))\n",
        "                    if all(edge_flow.get(edge, 0) <= f_thresh for edge in incident_edges):\n",
        "                        active_nodes.append(node)\n",
        "            \n",
        "            b0, b1, b2 = compute_betti_numbers_for_active(active_nodes, graph)\n",
        "            betti0_surf[iV, iF] = b0\n",
        "            betti1_surf[iV, iF] = b1\n",
        "            betti2_surf[iV, iF] = b2\n",
        "    \n",
        "    # Vectorize surfaces\n",
        "    feat = vectorize_betti_surfaces(betti0_surf, betti1_surf, betti2_surf)\n",
        "    features.append(feat)\n",
        "\n",
        "features = np.array(features)\n",
        "labels = np.array(labels)\n",
        "\n",
        "print(f\"\\nFeature matrix shape: {features.shape}\")\n",
        "print(f\"Labels shape: {labels.shape}\")\n",
        "print(f\"Feature statistics:\")\n",
        "print(f\"  Min: {features.min():.2f}\")\n",
        "print(f\"  Max: {features.max():.2f}\")\n",
        "print(f\"  Mean: {features.mean():.2f}\")\n",
        "print(f\"  Std: {features.std():.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7. Train XGBoost Classifier\n",
        "\n",
        "We'll train an XGBoost classifier on the multipersistence features.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    features, labels, test_size=0.2, random_state=42, stratify=labels\n",
        ")\n",
        "\n",
        "print(f\"Training set: {X_train.shape[0]} samples\")\n",
        "print(f\"Test set: {X_test.shape[0]} samples\")\n",
        "print(f\"Training class distribution: {np.bincount(y_train)}\")\n",
        "print(f\"Test class distribution: {np.bincount(y_test)}\")\n",
        "\n",
        "# Hyperparameter tuning\n",
        "param_grid = {\n",
        "    \"max_depth\": [3, 5, 7],\n",
        "    \"booster\": ['gbtree', 'dart'],\n",
        "    \"eta\": [0.1, 0.3, 0.5],\n",
        "    \"tree_method\": [\"auto\", \"exact\", \"approx\"],\n",
        "    \"subsample\": [0.8, 0.9, 1.0],\n",
        "    \"colsample_bytree\": [0.8, 0.9, 1.0]\n",
        "}\n",
        "\n",
        "print(\"\\nPerforming hyperparameter tuning...\")\n",
        "grid = GridSearchCV(\n",
        "    XGBClassifier(random_state=42, eval_metric='logloss'),\n",
        "    param_grid,\n",
        "    cv=5,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "grid.fit(X_train, y_train)\n",
        "print(f\"\\nBest parameters: {grid.best_params_}\")\n",
        "print(f\"Best CV score: {grid.best_score_:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 8. Evaluate Model Performance\n",
        "\n",
        "We'll evaluate the trained model using multiple metrics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train final model with best parameters\n",
        "best_model = grid.best_estimator_\n",
        "\n",
        "# Make predictions\n",
        "y_pred = best_model.predict(X_test)\n",
        "y_pred_proba = best_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "balanced_acc = balanced_accuracy_score(y_test, y_pred)\n",
        "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Cross-validation\n",
        "cv_scores = cross_val_score(best_model, features, labels, cv=10, scoring='accuracy')\n",
        "\n",
        "print(\"=== MULTIPERSISTENCE RESULTS ===\")\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Balanced Accuracy: {balanced_acc:.4f}\")\n",
        "print(f\"ROC AUC: {roc_auc:.4f}\")\n",
        "print(f\"Mean CV Accuracy: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
        "print(f\"\\nConfusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "\n",
        "# Feature importance\n",
        "feature_importance = best_model.feature_importances_\n",
        "print(f\"\\nTop 10 Most Important Features:\")\n",
        "top_features = np.argsort(feature_importance)[-10:][::-1]\n",
        "for i, feat_idx in enumerate(top_features):\n",
        "    print(f\"{i+1:2d}. Feature {feat_idx:3d}: {feature_importance[feat_idx]:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 9. Visualization\n",
        "\n",
        "We'll create visualizations to understand the multipersistence features.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot feature importance\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.bar(range(len(feature_importance)), feature_importance)\n",
        "plt.title('Feature Importance')\n",
        "plt.xlabel('Feature Index')\n",
        "plt.ylabel('Importance')\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Plot threshold distributions\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.hist(all_voltage_vals, bins=30, alpha=0.7, color='blue')\n",
        "plt.axvline(np.mean(all_voltage_vals), color='red', linestyle='--', label='Mean')\n",
        "plt.title('Voltage (ActiveLoad) Distribution')\n",
        "plt.xlabel('Voltage Value')\n",
        "plt.ylabel('Frequency')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.hist(all_flow_vals, bins=30, alpha=0.7, color='green')\n",
        "plt.axvline(np.mean(all_flow_vals), color='red', linestyle='--', label='Mean')\n",
        "plt.title('Branch Flow (Capacity) Distribution')\n",
        "plt.xlabel('Flow Value')\n",
        "plt.ylabel('Frequency')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n=== SUMMARY ===\")\n",
        "print(f\"Multipersistence analysis completed successfully!\")\n",
        "print(f\"- Used {len(thr_voltage)} voltage thresholds and {len(thr_flow)} flow thresholds\")\n",
        "print(f\"- Generated {features.shape[1]} features per graph\")\n",
        "print(f\"- Achieved {accuracy:.4f} accuracy on test set\")\n",
        "print(f\"- Cross-validation accuracy: {cv_scores.mean():.4f} ± {cv_scores.std():.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
